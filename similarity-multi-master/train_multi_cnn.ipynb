{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_multi_cnn.py\n",
    "#usage:python3 train_multi_cnn.py\n",
    "\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Dropout, Masking, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Bidirectional, Input,Dropout\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from gensim.models import word2vec\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CWE分类：根据已有数据集，分为以下类别：\n",
    "\t1. buffer_read_or_write_out_of_bounds\t'CWE121', 'CWE122', 'CWE123', 'CWE124', 'CWE126', 'CWE127'\n",
    "\t2. integer_overflow\t'CWE190'\n",
    "\t3. integer_underflow\t'CWE191'\n",
    "\t4. mismatched_memory_management_routines\t'CWE762'\n",
    "\t5. free_memory_not_on_heap\t'CWE590'\n",
    "\t6. resource_and_memory_exhaustion\t'CWE400', 'CWE401', 'CWE404'\n",
    "\t7. Use of externally-controlled format string\t'CWE134'\n",
    "\t8. problems_with_signed_to_unsigned_transformation_and_numeric_lenth\t'CWE194', 'CWE195', 'CWE196', 'CWE197'\n",
    "\t9. use_of_uninitialized_variable\t'CWE457'\n",
    "\t10. problems_during_free_operation\t'CWE415', 'CWE416'\n",
    "\t11. others\n",
    "\"\"\"\n",
    "\t\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "#参数设定\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 800\n",
    "EMBEDDING_DIM = 30\n",
    "\n",
    "\n",
    "\n",
    "path = \"sard_10_classes/\"\n",
    "data,label = get_original_data(path)\n",
    "\n",
    "\n",
    "print(\"start tokenizer\")\n",
    "print(\"number of data\",len(data))\n",
    "#print(data[0][0:4])\n",
    "print(\"exmaple of data:\\n\",data[0])\n",
    "#print(type(data[0][0:4]))\n",
    "\n",
    "class Attention(Layer):\n",
    "\tdef __init__(self, step_dim=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t W_regularizer=None, b_regularizer=None,\n",
    "\t\t\t\t W_constraint=None, b_constraint=None,\n",
    "\t\t\t\t bias=True, **kwargs):\n",
    "\t\tself.supports_masking = True\n",
    "\t\tself.init = initializers.get('glorot_uniform')\n",
    "\n",
    "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
    "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "\t\tself.W_constraint = constraints.get(W_constraint)\n",
    "\t\tself.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "\t\tself.bias = bias\n",
    "\t\tself.step_dim = step_dim\n",
    "\t\tself.features_dim = 0\n",
    "\t\tsuper(Attention, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tassert len(input_shape) == 3\n",
    "\n",
    "\t\tself.W = self.add_weight(shape=(input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t initializer=self.init,\n",
    "\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n",
    "\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n",
    "\t\tself.features_dim = input_shape[-1]\n",
    "\n",
    "\t\tif self.bias:\n",
    "\t\t\tself.b = self.add_weight(shape=(input_shape[1],),\n",
    "\t\t\t\t\t\t\t\t\t initializer='zero',\n",
    "\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n",
    "\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n",
    "\t\telse:\n",
    "\t\t\tself.b = None\n",
    "\n",
    "\t\tself.built = True\n",
    "\n",
    "\tdef compute_mask(self, input, input_mask=None):\n",
    "\t\treturn None\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\tfeatures_dim = self.features_dim\n",
    "\t\tstep_dim = self.step_dim\n",
    "\n",
    "\t\teij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "\t\t\t\t\t\tK.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "\t\tif self.bias:\n",
    "\t\t\teij += self.b\n",
    "\n",
    "\t\teij = K.tanh(eij)\n",
    "\n",
    "\t\ta = K.exp(eij)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\ta *= K.cast(mask, K.floatx())\n",
    "\n",
    "\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "\t\ta = K.expand_dims(a)\n",
    "\t\tweighted_input = x * a\n",
    "\t\treturn K.sum(weighted_input, axis=1)\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn input_shape[0],  self.features_dim\n",
    "\n",
    "#建立字典\n",
    "word_index = create_dictionary(data)\n",
    "write_data_to_json(word_index,'word_index.json')\n",
    "print(\"save dictionary word_index successfully\")\n",
    "\n",
    "#sequences\n",
    "sequences = get_sequences(data,word_index)\n",
    "\n",
    "#print('word_index is:',word_index)\n",
    "print(\"example of data transferred to indexes:\\n\",sequences[0])\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "\n",
    "pad_sequence,pad_label = pad_zero_to_sequences(sequences,label,MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#把序列中的词编号转换成字符串，为word_vec做准备\n",
    "for i in range(len(pad_sequence)):\n",
    "\tfor j in range(len(pad_sequence[i])):\n",
    "\t\tpad_sequence[i][j] = str(pad_sequence[i][j])\n",
    "\n",
    "#将pad_sequence,pad_label写入文件\n",
    "write_data_to_json(pad_sequence,'pad_sequence.json')\n",
    "write_data_to_json(pad_label,'pad_label.json')\n",
    "\n",
    "\n",
    "print(\"start word2vec\")\n",
    "model = word2vec.Word2Vec(pad_sequence, min_count=3, size=EMBEDDING_DIM)\n",
    "model.save(\"word.model\")\n",
    "\n",
    "#model = word2vec.Word2Vec.load(\"word.model\")\n",
    "\n",
    "print(\"creating enbedding index\")\n",
    "embeddings_index = {}\n",
    "nb_words = len(word_index)+1\n",
    "word_vectors = model.wv\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "\tif int(vocab_obj.index) < nb_words:\n",
    "\t\tembeddings_index[word] = word_vectors[word]\n",
    "#print(embeddings_index)\n",
    "print(\"creating embedding matrix\")\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "\tif i >= nb_words:\n",
    "\t\tcontinue\n",
    "\tembedding_vector = embeddings_index.get(str(i))\n",
    "\t#print(embedding_vector)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "print(\"for example,the first 5 words in dictionary will be transferred to this vector:\\n\",embedding_matrix[0:5])\n",
    "print(\"example of sequence after padding\",pad_sequence[0])\n",
    "\n",
    "\n",
    "#将训练集中的标签为0的样本进行采样，保留7000个。\n",
    "pos = []\n",
    "pos_label = []\n",
    "neg = []\n",
    "for i in range(len(pad_sequence)):\n",
    "\tif pad_label[i] == 0:\n",
    "\t\tneg.append(pad_sequence[i])\n",
    "\telse:\n",
    "\t\tpos.append(pad_sequence[i])\n",
    "\t\tpos_label.append(pad_label[i])\n",
    "neg = random.sample(neg, 7000)\n",
    "pos.extend(neg)\n",
    "pos_label.extend([0]*7000)\n",
    "\n",
    "pad_sequence = pos\n",
    "pad_label = pos_label\n",
    "\n",
    "seed = 4396\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(pad_sequence)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(pad_label)\n",
    "print('已经对标签为0的样本进行欠采样，pad_sequence长度：',len(pad_sequence),',pad_label长度：',len(pad_label))\n",
    "\n",
    "\n",
    "#划分训练集和测试集\n",
    "\n",
    "split_number = int(len(pad_sequence)*0.8)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(pairs, pair_labels, test_size=0.1, random_state=0)\n",
    "\n",
    "x_train = pad_sequence[:split_number]\n",
    "x_test = pad_sequence[split_number:]\n",
    "y_train = pad_label[:split_number]\n",
    "y_test = pad_label[split_number:]\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"number of training data:\",len(x_train))\n",
    "print(\"number of testing data:\",len(x_test))\n",
    "\n",
    "\n",
    "print(\"creating model\")\n",
    "\n",
    "def get_model():\n",
    "\n",
    "\tembedding_layer = Embedding(nb_words,\n",
    "\t\t\t\t\t\t\t\tEMBEDDING_DIM,\n",
    "\t\t\t\t\t\t\t\tweights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\t\ttrainable=False)\n",
    "\n",
    "\tmain_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\tembed = embedding_layer(main_input)\n",
    "\t\n",
    "\n",
    "\t# 词窗大小分别为3,4,5\n",
    "\tcnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "\tcnn1 = MaxPooling1D(pool_size=EMBEDDING_DIM)(cnn1)\n",
    "\tcnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "\tcnn2 = MaxPooling1D(pool_size=EMBEDDING_DIM)(cnn2)\n",
    "\tcnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "\tcnn3 = MaxPooling1D(pool_size=EMBEDDING_DIM)(cnn3)\n",
    "\t# 合并三个模型的输出向量\n",
    "\tcnn = Concatenate(axis=-1)([cnn1, cnn2, cnn3])\n",
    "\tflat = Flatten()(cnn)\n",
    "\tdrop = Dropout(0.2)(flat)\n",
    "\tmain_output = Dense(11, activation='softmax')(drop)\n",
    "\n",
    "\n",
    "\tmodel = Model(inputs=main_input, outputs=main_output)\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t\t\t\t  optimizer='adam',\n",
    "\t\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "\treturn model\n",
    "\n",
    "print(\"shape of training data, format is (number of data,lenth of one sequence):\",x_train.shape)\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "\tdef on_train_begin(self, logs={}):\n",
    "\t\tself.val_f1s = []\n",
    "\t\tself.val_recalls = []\n",
    "\t\tself.val_precisions = []\n",
    "\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\tval_predict = np.argmax(np.asarray(self.model.predict(self.validation_data[0])), axis=1)\n",
    "\t\tval_targ = np.argmax(self.validation_data[1], axis=1)\n",
    "\t\t_val_f1 = f1_score(val_targ, val_predict, average='macro')\n",
    "\t\t#_val_recall = recall_score(val_targ, val_predict, average='binary')\n",
    "\t\t#_val_precision = precision_score(val_targ, val_predict, average='binary')\n",
    "\t\tself.val_f1s.append(_val_f1)\n",
    "\t\t#self.val_recalls.append(_val_recall)\n",
    "\t\t#self.val_precisions.append(_val_precision)\n",
    "\t\tprint(\"— _val_f1: %f \"%_val_f1)\n",
    "\t\t#print(\"— _val_recall: %f \"%_val_recall)\n",
    "\t\t#print(\"— _val_precision: %f \"%_val_precision)\n",
    "\t\treturn\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "one_hot_labels_train = to_categorical(y_train, num_classes=11)  # 将标签转换为one-hot编码\n",
    "one_hot_labels_test = to_categorical(y_test, num_classes=11)  # 将标签转换为one-hot编码\n",
    "model.fit(x_train, one_hot_labels_train,\n",
    "\t\t  batch_size=32,\n",
    "\t\t  epochs=5,\n",
    "\t\t  class_weight='auto',\n",
    "\t\t  validation_data=(x_test, one_hot_labels_test),\n",
    "\t\t  callbacks =[metrics])\n",
    "print(\"saving model...\")\n",
    "model.save('cnn_model.h5')\n",
    "print(\"model has been saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
