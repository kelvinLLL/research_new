{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load retrain_multi.py\n",
    "#使用：python3 retrain_multi.py target_classification\n",
    "#再训练：用户将新增的自定义训练数据放入目录add_sample/下，之后重新训练模型。\n",
    "#样本文件名要求：文件名中包含cwe编号，并且包含good/bad标签。cwe编号按照预设的定义划入预设的10个分类，如果cwe编号不在这10类中，则划分为11类（其他类）。\n",
    "#使用方法：新增用户数据后，对每一个目标类运行该脚本：python3 retrain_multi.py target_classification。其中target_classification是要训练的模型专门识别的类的编号（1-11）。\n",
    "#可以将target_classification从1到11全部再训练一遍，也可以只再训练新增类对应的模型。\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Concatenate, Dropout, Masking, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "#from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Bidirectional, Input,Dropout\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from gensim.models import word2vec\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CWE分类：根据已有数据集，分为以下类别：\n",
    "\t1. buffer_read_or_write_out_of_bounds\t'CWE121', 'CWE122', 'CWE123', 'CWE124', 'CWE126', 'CWE127'\n",
    "\t2. integer_overflow\t'CWE190'\n",
    "\t3. integer_underflow\t'CWE191'\n",
    "\t4. mismatched_memory_management_routines\t'CWE762'\n",
    "\t5. free_memory_not_on_heap\t'CWE590'\n",
    "\t6. resource_and_memory_exhaustion\t'CWE400', 'CWE401', 'CWE404'\n",
    "\t7. Use of externally-controlled format string\t'CWE134'\n",
    "\t8. problems_with_signed_to_unsigned_transformation_and_numeric_lenth\t'CWE194', 'CWE195', 'CWE196', 'CWE197'\n",
    "\t9. use_of_uninitialized_variable\t'CWE457'\n",
    "\t10. problems_during_free_operation\t'CWE415', 'CWE416'\n",
    "\t11. others\n",
    "\"\"\"\n",
    "\n",
    "#不输出warning信息\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#禁用gpu，强制使用cpu（因为nvidia驱动版本太老，尚未更新，临时设置此项）\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "#参数设定\n",
    "num_lstm1 = 64\n",
    "num_dense = 64\n",
    "rate_drop_lstm = 0.1\n",
    "rate_drop_dense = 0.1\n",
    "MAX_SEQUENCE_LENGTH = 800\n",
    "EMBEDDING_DIM = 30\n",
    "split_number = 80000\n",
    "\n",
    "path = \"sard_10_classes/\"\n",
    "data,label = get_original_data(path)\n",
    "path = \"add_sample/\"\n",
    "data_added,label_added = get_original_data(path)\n",
    "data = data + data_added\n",
    "label = label + label_added\n",
    "\n",
    "print(\"start tokenizer\")\n",
    "print(\"number of data\",len(data))\n",
    "#print(data[0][0:4])\n",
    "print(\"exmaple of data:\\n\",data[0])\n",
    "#print(type(data[0][0:4]))\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\tdef __init__(self, step_dim=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t W_regularizer=None, b_regularizer=None,\n",
    "\t\t\t\t W_constraint=None, b_constraint=None,\n",
    "\t\t\t\t bias=True, **kwargs):\n",
    "\t\tself.supports_masking = True\n",
    "\t\tself.init = initializers.get('glorot_uniform')\n",
    "\n",
    "\t\tself.W_regularizer = regularizers.get(W_regularizer)\n",
    "\t\tself.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "\t\tself.W_constraint = constraints.get(W_constraint)\n",
    "\t\tself.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "\t\tself.bias = bias\n",
    "\t\tself.step_dim = step_dim\n",
    "\t\tself.features_dim = 0\n",
    "\t\tsuper(Attention, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tassert len(input_shape) == 3\n",
    "\n",
    "\t\tself.W = self.add_weight(shape=(input_shape[-1],),\n",
    "\t\t\t\t\t\t\t\t initializer=self.init,\n",
    "\t\t\t\t\t\t\t\t name='{}_W'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t regularizer=self.W_regularizer,\n",
    "\t\t\t\t\t\t\t\t constraint=self.W_constraint)\n",
    "\t\tself.features_dim = input_shape[-1]\n",
    "\n",
    "\t\tif self.bias:\n",
    "\t\t\tself.b = self.add_weight(shape=(input_shape[1],),\n",
    "\t\t\t\t\t\t\t\t\t initializer='zero',\n",
    "\t\t\t\t\t\t\t\t\t name='{}_b'.format(self.name),\n",
    "\t\t\t\t\t\t\t\t\t regularizer=self.b_regularizer,\n",
    "\t\t\t\t\t\t\t\t\t constraint=self.b_constraint)\n",
    "\t\telse:\n",
    "\t\t\tself.b = None\n",
    "\n",
    "\t\tself.built = True\n",
    "\n",
    "\tdef compute_mask(self, input, input_mask=None):\n",
    "\t\treturn None\n",
    "\n",
    "\tdef call(self, x, mask=None):\n",
    "\t\tfeatures_dim = self.features_dim\n",
    "\t\tstep_dim = self.step_dim\n",
    "\n",
    "\t\teij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "\t\t\t\t\t\tK.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "\t\tif self.bias:\n",
    "\t\t\teij += self.b\n",
    "\n",
    "\t\teij = K.tanh(eij)\n",
    "\n",
    "\t\ta = K.exp(eij)\n",
    "\n",
    "\t\tif mask is not None:\n",
    "\t\t\ta *= K.cast(mask, K.floatx())\n",
    "\n",
    "\t\ta /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "\t\ta = K.expand_dims(a)\n",
    "\t\tweighted_input = x * a\n",
    "\t\treturn K.sum(weighted_input, axis=1)\n",
    "\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn input_shape[0],  self.features_dim\n",
    "\n",
    "\n",
    "\n",
    "#建立字典\n",
    "if os.path.isfile('word_index.json'):\n",
    "\tword_index = read_data_from_json('word_index.json')\n",
    "else:\n",
    "\tword_index = create_dictionary(data)\n",
    "\twrite_data_to_json(word_index,'word_index.json')\n",
    "\tprint(\"save dictionary word_index successfully\")\n",
    "\n",
    "#sequences\n",
    "sequences = get_sequences(data,word_index)\n",
    "#print('word_index is:',word_index)\n",
    "print(\"example of data transferred to indexes:\\n\",sequences[0])\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "pad_sequence,pad_label = pad_zero_to_sequences(sequences,label,MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#把序列中的词编号转换成字符串，为word_vec做准备\n",
    "for i in range(len(pad_sequence)):\n",
    "\tfor j in range(len(pad_sequence[i])):\n",
    "\t\tpad_sequence[i][j] = str(pad_sequence[i][j])\n",
    "\n",
    "#将pad_sequence,pad_label写入文件\n",
    "write_data_to_json(pad_sequence,'pad_sequence.json')\n",
    "write_data_to_json(pad_label,'pad_label.json')\n",
    "\n",
    "\n",
    "print(\"start word2vec\")\n",
    "model = word2vec.Word2Vec(pad_sequence, min_count=3, size=EMBEDDING_DIM)\n",
    "model.save(\"word.model\")\n",
    "\n",
    "#model = word2vec.Word2Vec.load(\"word.model\")\n",
    "\n",
    "print(\"creating enbedding index\")\n",
    "embeddings_index = {}\n",
    "nb_words = len(word_index)+1\n",
    "word_vectors = model.wv\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "\tif int(vocab_obj.index) < nb_words:\n",
    "\t\tembeddings_index[word] = word_vectors[word]\n",
    "#print(embeddings_index)\n",
    "print(\"creating embedding matrix\")\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "\tif i >= nb_words:\n",
    "\t\tcontinue\n",
    "\tembedding_vector = embeddings_index.get(str(i))\n",
    "\t#print(embedding_vector)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector\n",
    "print(\"for example,the first 5 words in dictionary will be transferred to this vector:\\n\",embedding_matrix[0:5])\n",
    "print(\"example of sequence after padding\",pad_sequence[0])\n",
    "\n",
    "\n",
    "#根据标签区分正负样本\n",
    "pos = []\n",
    "neg = []\n",
    "print(\"length of pad_sequence:\",len(pad_sequence))\n",
    "print(\"length of pad_label:\",len(pad_label))\n",
    "target_classification = int(sys.argv[1])\n",
    "for i in range(len(pad_sequence)):\n",
    "\tif pad_label[i] == target_classification:\n",
    "\t\tpos.append(pad_sequence[i])\n",
    "\telse:\n",
    "\t\tneg.append(pad_sequence[i])\n",
    "print(\"positive:\",len(pos),\"negative:\",len(neg))\n",
    "#p_sample = random.sample(pos, 10000)\n",
    "#n_sample = random.sample(neg, 10000)\n",
    "print(\"creating pairs\")\n",
    "pairs_1 = []\n",
    "pairs_2 = []\n",
    "pair_labels = []\n",
    "for i in range(50000):\n",
    "\tx1 = random.choice(pos)\n",
    "\ty1 = random.choice(pos)\n",
    "\tpairs_1.append(x1)\n",
    "\tpairs_2.append(y1)\n",
    "\tpair_labels.append(1)\n",
    "\n",
    "for i in range(50000):\n",
    "\tx1 = random.choice(neg)\n",
    "\ty1 = random.choice(pos)\n",
    "\tpairs_1.append(x1)\n",
    "\tpairs_2.append(y1)\n",
    "\tpair_labels.append(0)\n",
    "\n",
    "\n",
    "seed = 4396\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(pairs_1)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(pairs_2)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(pair_labels)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(pairs, pair_labels, test_size=0.1, random_state=0)\n",
    "x_train_1 = pairs_1[:split_number]\n",
    "x_train_2 = pairs_2[:split_number]\n",
    "x_test_1 = pairs_1[split_number:]\n",
    "x_test_2 = pairs_2[split_number:]\n",
    "y_train = pair_labels[:split_number]\n",
    "y_test = pair_labels[split_number:]\n",
    "\n",
    "x_train_1 = np.array(x_train_1)\n",
    "x_train_2 = np.array(x_train_2)\n",
    "y_train = np.array(y_train)\n",
    "x_test_1 = np.array(x_test_1)\n",
    "x_test_2 = np.array(x_test_2)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"number of training data:\",len(x_train_1))\n",
    "print(\"number of testing data:\",len(x_test_1))\n",
    "\n",
    "\n",
    "print(\"creating model\")\n",
    "def get_model():\n",
    "#\tmasking_layer = Masking(mask_value=-1, input_shape=(200, 100))\n",
    "\tembedding_layer = Embedding(nb_words,\n",
    "\t\t\t\t\t\t\t\tEMBEDDING_DIM,\n",
    "\t\t\t\t\t\t\t\tweights=[embedding_matrix],\n",
    "\t\t\t\t\t\t\t\tinput_length=MAX_SEQUENCE_LENGTH,\n",
    "\t\t\t\t\t\t\t\ttrainable=False)\n",
    "\tfirst_lstm_layer1 = Bidirectional(LSTM(num_lstm1, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True))\n",
    "#\tsecond_lstm_layer1 = Bidirectional(LSTM(num_lstm2, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\tfirst_lstm_layer2 = Bidirectional(LSTM(num_lstm1, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True))\n",
    "#\tsecond_lstm_layer2 = Bidirectional(LSTM(num_lstm2, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "#\tthird_lstm_layer = Bidirectional(LSTM(num_lstm3, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True))\n",
    "#\tfourth_lstm_layer = Bidirectional(LSTM(num_lstm4, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "\n",
    "\tinput_1 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\tembedded_sequences_1 = embedding_layer(input_1)\n",
    "\tsequence_1_input = Masking(mask_value=0, input_shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(embedded_sequences_1)\n",
    "\tfirst_y1 = first_lstm_layer1(sequence_1_input)\n",
    "\ty1 =  Attention(MAX_SEQUENCE_LENGTH)(first_y1)\n",
    "#\tthird_y1 = third_lstm_layer(second_y1)\n",
    "#\ty1 = fourth_lstm_layer(third_y1)\n",
    "#\ty1 = Dense(num_lstm4,activation = 'relu')(y1)\n",
    "\n",
    "\tinput_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\tembedded_sequences_2 = embedding_layer(input_2)\n",
    "\tsequence_2_input = Masking(mask_value=0, input_shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM))(embedded_sequences_2)\n",
    "\tfirst_y2 = first_lstm_layer2(sequence_2_input)\n",
    "\ty2 = Attention(MAX_SEQUENCE_LENGTH)(first_y2)\n",
    "#\tthird_y2 = third_lstm_layer(second_y2)\n",
    "#\ty2 = fourth_lstm_layer(third_y2)\n",
    "#\ty2 = Dense(num_lstm4,activation = 'relu')(y2)\n",
    "\n",
    "\tmerged = Concatenate(axis = -1)([y1, y2])\n",
    "\tmerged = Dropout(rate_drop_dense)(merged)\n",
    "\tmerged = BatchNormalization()(merged)\n",
    "\n",
    "\tmerged = Dense(num_dense, activation='relu')(merged)\n",
    "\tmerged = Dropout(rate_drop_dense)(merged)\n",
    "#\tmerged = BatchNormalization()(merged)\n",
    "#\tmerged = Dense(16, activation='relu')(merged)\n",
    "#\tmerged = Dropout(rate_drop_dense)(merged)\n",
    "\tmerged = BatchNormalization()(merged)\n",
    "#\tmerged = Dense(num_dense_2, activation='relu')(merged)\n",
    "\tpreds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\tmodel = Model(inputs=[input_1, input_2], outputs=preds)\n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "\t\t\t\t  optimizer='adam',\n",
    "\t\t\t\t  metrics=['acc'])\n",
    "\n",
    "\treturn model\n",
    "\n",
    "print(\"shape of training data, format is (number of data,lenth of one sequence):\",x_train_1.shape)\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "\tdef on_train_begin(self, logs={}):\n",
    "\t\tself.val_f1s = []\n",
    "\t\tself.val_recalls = []\n",
    "\t\tself.val_precisions = []\n",
    "\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\tval_predict=(np.asarray(self.model.predict([x_test_1,x_test_2]))).round()\n",
    "\t\tval_targ = y_test\n",
    "\t\t_val_f1 = f1_score(val_targ, val_predict, average='binary')\n",
    "\t\t_val_recall = recall_score(val_targ, val_predict, average='binary')\n",
    "\t\t_val_precision = precision_score(val_targ, val_predict, average='binary')\n",
    "\t\tself.val_f1s.append(_val_f1)\n",
    "\t\tself.val_recalls.append(_val_recall)\n",
    "\t\tself.val_precisions.append(_val_precision)\n",
    "\t\tprint(\"— _val_f1: %f \"%_val_f1)\n",
    "\t\tprint(\"— _val_recall: %f \"%_val_recall)\n",
    "\t\tprint(\"— _val_precision: %f \"%_val_precision)\n",
    "\t\treturn\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.fit([x_train_1, x_train_2], y_train,\n",
    "\t\t  batch_size=32,\n",
    "\t\t  epochs=5,\n",
    "\t\t  validation_data=([x_test_1,x_test_2], y_test),\n",
    "\t\t  callbacks =[metrics])\n",
    "print(\"saving model...\")\n",
    "model.save('models/'+str(sys.argv[1])+'_bilstm_retrained_model.h5')\n",
    "print(\"model has been saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
